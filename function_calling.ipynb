{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparando ambiente\n",
    "import json\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caminho lógico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um problema dos modelos de linguagem é a atualização dos dados. No caso do GPT, os modelos não tem acesso a internet e sua informação sempre tem um teto temporal.\n",
    "Uma maneira de lidar com essa questão é oferecer ao modelo algumas ferramentas (tools) para que possam buscar essas informações de outras maneiras. um exemplo, é oferecer ao modelo algumas funções que podem ser acessada com parâmetros.\n",
    "Como estamos falando de um modelo de linguagem, essas funções funcionam melhor quando o nome descreve bem a sua funcionalidade, assim como os seus parâmetros.\n",
    "\n",
    "Imagine que tenhamos uma base de dados com informações sobre a temperatura em determinados locais. Podemos utilizar essa informação no modelo, permitindo que ele acesse esses dados por meio de uma função."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de apoio (tool)\n",
    "Vamos definir essa função como se segue. A função recebe dois parâmetros (local e a unidade) e retorna uma **string** de um dicionário com as chaves \"local\", \"temperatura\" e \"unidade\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo uma função que possa retornar a temperatura de \n",
    "# determinados locais\n",
    "\n",
    "def obter_temperatura_atual(local, unidade=\"celsius\"): # o nome da função é importante para que o modelo entenda o que ela faz\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"rio de janeiro\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Rio de Janeiro\", \"temperatura\": \"35\", \"unidade\": unidade}\n",
    "            )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar algumas saídas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"local\": \"S\\u00e3o Paulo\", \"temperatura\": \"32\", \"unidade\": \"celsius\"}\n"
     ]
    }
   ],
   "source": [
    "test = obter_temperatura_atual('São Paulo')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"local\": \"Pen\\u00e1polis\", \"temperatura\": \"unknown\"}\n"
     ]
    }
   ],
   "source": [
    "test = obter_temperatura_atual('Penápolis')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando a função não conhece a temperatura do lugar ela devolve a string informando unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando o algorítmo com informações atuais\n",
    "Agora, vamos fazer uma requisição da maneira que já conhecemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "messagens = [{'role':'user','content':'Qual a temperatura em São Paulo neste momento?'}]\n",
    "\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    messages=messagens,\n",
    "    max_tokens=100, \n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando a resposta do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desculpe, mas não consigo fornecer informações em tempo real, como a temperatura atual em São Paulo. Recomendo verificar um site de meteorologia ou um aplicativo de clima para obter as informações mais recentes.\n"
     ]
    }
   ],
   "source": [
    "print(resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que o modelo nos informa de que não tem informações em tempo real. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendo a lista de ferramentas (tools)\n",
    "Seguindo o exemplo anterior, podemos permitir que o modelo utilize a nossa função `obter_temperatura_atual` indicando um parâmetro `tools` (uma lista de ferramentas disponíveis) e `tool_choice` (a maneira como o modelo pode escolher as ferramentas).\n",
    "\n",
    "Antes, vamos então denifir a nossa lista de ferramentas, cada ferramenta disponível será informada no formato de dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo a lista de tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"obter_temperatura_atual\",\n",
    "            \"description\": \"Obtém a temperatura atual em uma dada cidade\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"local\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"O nome da cidade. Ex: São Paulo\",\n",
    "                    },\n",
    "                    \"unidade\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"local\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1\n"
     ]
    }
   ],
   "source": [
    "# entendendo o objeto tools\n",
    "print(type(tools), len(tools))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percebemos que o tools é uma lista de 1 elemento (nossa função de obter a temperatura). Essa lista \"explica\" para o modelo o que é cada ferramenta disponibilizada, quais são seus parâmetros e outros detalhes.\n",
    "\n",
    "Vamos explorar a função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao = tools[0]\n",
    "type(funcao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'function'])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keys da funcao\n",
    "funcao.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As keys da função são `type` e `function`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'function'"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'obter_temperatura_atual',\n",
       " 'description': 'Obtém a temperatura atual em uma dada cidade',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'local': {'type': 'string',\n",
       "    'description': 'O nome da cidade. Ex: São Paulo'},\n",
       "   'unidade': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}},\n",
       "  'required': ['local']}}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao['function']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['name', 'description', 'parameters'])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcao['function'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'obter_temperatura_atual'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nome da função\n",
    "funcao['function']['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obtém a temperatura atual em uma dada cidade'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descrição da função\n",
    "funcao['function']['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'object',\n",
       " 'properties': {'local': {'type': 'string',\n",
       "   'description': 'O nome da cidade. Ex: São Paulo'},\n",
       "  'unidade': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}},\n",
       " 'required': ['local']}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parâmetros da função\n",
    "funcao['function']['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'properties', 'required'])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dentro dos parâmetros temos:\n",
    "funcao['function']['parameters'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tipo do parâmetro\n",
    "funcao['function']['parameters']['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entender porque é `object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'local': {'type': 'string', 'description': 'O nome da cidade. Ex: São Paulo'},\n",
       " 'unidade': {'type': 'string', 'enum': ['celsius', 'fahrenheit']}}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# propriedades do parâmetro\n",
    "funcao['function']['parameters']['properties']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro das propriedades dos parâmetros é que iremos indicar cada um dos parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['local', 'unidade'])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parâmetros das funções\n",
    "funcao['function']['parameters']['properties'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, dentro de cada um desses parâmetros, temos as informações:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['type', 'description'])\n",
      "string\n",
      "O nome da cidade. Ex: São Paulo\n"
     ]
    }
   ],
   "source": [
    "# definições do local (nome da cidade)\n",
    "print(funcao['function']['parameters']['properties']['local'].keys())\n",
    "print(funcao['function']['parameters']['properties']['local']['type'])\n",
    "print(funcao['function']['parameters']['properties']['local']['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['type', 'enum'])\n",
      "string\n",
      "['celsius', 'fahrenheit']\n"
     ]
    }
   ],
   "source": [
    "# definições da unidade (de medida da temperatura)\n",
    "print(funcao['function']['parameters']['properties']['unidade'].keys())\n",
    "print(funcao['function']['parameters']['properties']['unidade']['type'])\n",
    "print(funcao['function']['parameters']['properties']['unidade']['enum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `enum` é utilizada para definir categorias de uma variável."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizando as tools no modelo\n",
    "Agora sim, estamos prontos para interagir com o modelo permitindo que ele acesse funções:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desculpe, mas não consigo fornecer informações em tempo real, como a temperatura atual em São Paulo. Recomendo verificar um site de meteorologia ou um aplicativo de clima para obter as informações mais recentes.\n"
     ]
    }
   ],
   "source": [
    "# vamos repetir a pergunta anterior\n",
    "messages = [{'role':'user','content':'Qual a temperatura em São Paulo neste momento?'}]\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    messages=messages,\n",
    "    max_tokens=100, \n",
    "    temperature=0,\n",
    ")\n",
    "print(resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos refazer a pergunta passando os parâmetros com as `tools` e observar o que o modelo responde "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-A9HtC7pHPhDU5p06l3Ayhr3yrlDqg', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None))], created=1726777082, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=20, prompt_tokens=83, total_tokens=103, completion_tokens_details={'reasoning_tokens': 0}))\n"
     ]
    }
   ],
   "source": [
    "messages = [{'role':'user','content':'Qual a temperatura em São Paulo neste momento?'}]\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    tools=tools,\n",
    "    tool_choice='auto',\n",
    "    messages=messages,\n",
    "    max_tokens=100, \n",
    "    temperature=0,\n",
    ")\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando a resposta do modelo, percebemos que não há mensagem de retorno e aparentemente o motivo foi relacionado a tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tool_calls'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices[0].finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entendendo a resposta do modelo\n",
    "Dentro do `message` agora há o `tool_calls`, que nada mais é do que o modelo solicitando que a função `obter_temperatura_atual`, seja chamada com o argumento `\"local\":\"São Paulo\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chamando as funções solicitadas\n",
    "Então, precisamos chamar essa função com esse parâmetro e devolver esse valor para o modelo formular a sua resposta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None)\n",
      "[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# armazenando as solicitações de chamada em um objeto\n",
    "message_resp = resposta.choices[0].message\n",
    "tool_calls = message_resp.tool_calls\n",
    "print(message_resp)\n",
    "print(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')]\n"
     ]
    }
   ],
   "source": [
    "# verificando se há chamadas solicitadas pelo modelo:\n",
    "if tool_calls:\n",
    "    print(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adicionando a mensagem de resposta na lista de mensagens\n",
    "messages.append(message_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Qual a temperatura em São Paulo neste momento?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None)]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# o objeto tool_calls é uma lista, porque o modelo pode pedir para chamar várias tools:\n",
    "type(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')\n"
     ]
    }
   ],
   "source": [
    "# então, vamos iterar sobre cada uma das chamadas solicitadas:\n",
    "for tool_call in tool_calls:\n",
    "    print(tool_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, vamos iterar sobre cada uma das chamadas solicitadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obter_temperatura_atual\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    # pegar o nome da função\n",
    "    function_name = tool_call.function.name\n",
    "    print(function_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, precisamos executar essa função, pra isso também precisamos criar um dicionário que **leia** o nome da nossa função e **retorne** a função em si:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcoes_disponiveis = {\n",
    "        \"obter_temperatura_atual\": obter_temperatura_atual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function obter_temperatura_atual at 0x000001D233E64D60>\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    # pegar o nome da função\n",
    "    function_name = tool_call.function.name\n",
    "    function_to_call = funcoes_disponiveis[function_name]\n",
    "    print(function_to_call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também precisamos dos argumentos da função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'local': 'São Paulo'}\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    # pegar o nome da função\n",
    "    function_name = tool_call.function.name\n",
    "    function_to_call = funcoes_disponiveis[function_name]\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    print(function_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, só nos falta chamar a função:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"local\": \"S\\u00e3o Paulo\", \"temperatura\": \"32\", \"unidade\": null}\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    # pegar o nome da função\n",
    "    function_name = tool_call.function.name\n",
    "    # pegar a função \n",
    "    function_to_call = funcoes_disponiveis[function_name]\n",
    "    # argumentos da função\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    # chamando a função:\n",
    "    function_response = function_to_call(\n",
    "                local=function_args.get(\"local\"),\n",
    "                unidade=function_args.get(\"unidade\"),\n",
    "            )\n",
    "    print(function_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o resultado da função, vamos adicionar essa \"mensagem\" à nossa lista de mensagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Qual a temperatura em São Paulo neste momento?'}, ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None), {'tool_call_id': 'call_scYC3CPipM7xwiBvXoIPZUIB', 'role': 'tool', 'name': 'obter_temperatura_atual', 'content': '{\"local\": \"S\\\\u00e3o Paulo\", \"temperatura\": \"32\", \"unidade\": null}'}]\n"
     ]
    }
   ],
   "source": [
    "for tool_call in tool_calls:\n",
    "    # pegar o nome da função\n",
    "    function_name = tool_call.function.name\n",
    "    # pegar a função \n",
    "    function_to_call = funcoes_disponiveis[function_name]\n",
    "    # argumentos da função\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    # chamando a função:\n",
    "    function_response = function_to_call(\n",
    "                local=function_args.get(\"local\"),\n",
    "                unidade=function_args.get(\"unidade\"),\n",
    "            )\n",
    "    # adicionando a resposta da função à lista de mensagens\n",
    "    messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id, # importante passar o id da chamada\n",
    "                \"role\": \"tool\", # informamos que é uma mensagem de tool (diferente do que fizemos até agora com user e assistant)\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )\n",
    "    print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando o resultado das chamadas na requisição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após finalizar de adicionar todas as respostas das chamadas, podemos realizar uma nova \"pergunta\" ao GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "segunda_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages,\n",
    "        max_tokens=100, \n",
    "        temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E, finalmente, podemos ver a resposta do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessage(content='A temperatura em São Paulo neste momento é de 32 graus Celsius.', role='assistant', function_call=None, tool_calls=None, refusal=None)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segunda_resposta.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A temperatura em São Paulo neste momento é de 32 graus Celsius.'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segunda_resposta.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No fim das contas, o modelo foi capaz de incorporar novas informações (por meio de uma função) ao responder o usuário.\n",
    "Neste exemplo utilizei o parâmetro `temperature = 0`, mas funciona com outros valores também:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A temperatura em São Paulo neste momento é de 32 graus Celsius.'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segunda_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages,\n",
    "        max_tokens=100, \n",
    "        temperature=1,\n",
    "    )\n",
    "segunda_resposta.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mais de uma chamada para funções\n",
    "E o que acontece de pedir a temperatura de duas cidades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adicionando uma nova request com duas outras cidades\n",
    "messages.append({'role':'user','content':'E qual a temperatura em Porto Alegre e Rio de Janeiro?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Qual a temperatura em São Paulo neste momento?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None),\n",
       " {'tool_call_id': 'call_scYC3CPipM7xwiBvXoIPZUIB',\n",
       "  'role': 'tool',\n",
       "  'name': 'obter_temperatura_atual',\n",
       "  'content': '{\"local\": \"S\\\\u00e3o Paulo\", \"temperatura\": \"32\", \"unidade\": null}'},\n",
       " {'role': 'user',\n",
       "  'content': 'E qual a temperatura em Porto Alegre e Rio de Janeiro?'}]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-A9HtEdyWHGkB7n37hP3dNf7Cf2Pp1', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_FeVg6ZpiJC6JU3KLEzi4qwts', function=Function(arguments='{\"local\": \"Porto Alegre\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_ZFN0Acj5C9V1RjHM7XQemIIv', function=Function(arguments='{\"local\": \"Rio de Janeiro\"}', name='obter_temperatura_atual'), type='function')], refusal=None))], created=1726777084, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_1bb46167f9', usage=CompletionUsage(completion_tokens=58, prompt_tokens=157, total_tokens=215, completion_tokens_details={'reasoning_tokens': 0}))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fazendo a solicitação: \n",
    "terceira_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages,\n",
    "        max_tokens=100, \n",
    "        temperature=0,\n",
    "        tools=tools,\n",
    "        tool_choice='auto'\n",
    "    )\n",
    "terceira_resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novamente, o modelo nos solicita para rodar as funções, dessa vez, a lista de funções a serem rodadas tem dois objetos. A primeira com o argumento `\"local\": \"Porto Alegre\"` e a segunda com `\"local\": \"Rio de Janeiro\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_FeVg6ZpiJC6JU3KLEzi4qwts', function=Function(arguments='{\"local\": \"Porto Alegre\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_ZFN0Acj5C9V1RjHM7XQemIIv', function=Function(arguments='{\"local\": \"Rio de Janeiro\"}', name='obter_temperatura_atual'), type='function')], refusal=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_resp = terceira_resposta.choices[0].message\n",
    "tool_calls = message_resp.tool_calls\n",
    "print(message_resp)\n",
    "len(tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessageToolCall(id='call_FeVg6ZpiJC6JU3KLEzi4qwts', function=Function(arguments='{\"local\": \"Porto Alegre\"}', name='obter_temperatura_atual'), type='function')"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessageToolCall(id='call_ZFN0Acj5C9V1RjHM7XQemIIv', function=Function(arguments='{\"local\": \"Rio de Janeiro\"}', name='obter_temperatura_atual'), type='function')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_calls[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos chamar novamente essas funções com os novos parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Qual a temperatura em São Paulo neste momento?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_scYC3CPipM7xwiBvXoIPZUIB', function=Function(arguments='{\"local\":\"São Paulo\"}', name='obter_temperatura_atual'), type='function')], refusal=None),\n",
       " {'tool_call_id': 'call_scYC3CPipM7xwiBvXoIPZUIB',\n",
       "  'role': 'tool',\n",
       "  'name': 'obter_temperatura_atual',\n",
       "  'content': '{\"local\": \"S\\\\u00e3o Paulo\", \"temperatura\": \"32\", \"unidade\": null}'},\n",
       " {'role': 'user',\n",
       "  'content': 'E qual a temperatura em Porto Alegre e Rio de Janeiro?'},\n",
       " ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_FeVg6ZpiJC6JU3KLEzi4qwts', function=Function(arguments='{\"local\": \"Porto Alegre\"}', name='obter_temperatura_atual'), type='function'), ChatCompletionMessageToolCall(id='call_ZFN0Acj5C9V1RjHM7XQemIIv', function=Function(arguments='{\"local\": \"Rio de Janeiro\"}', name='obter_temperatura_atual'), type='function')], refusal=None)]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adicionando a mensagem de chamada das requisições\n",
    "messages.append(message_resp)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chamando as funções\n",
    "for tool_call in tool_calls:\n",
    "    # pegar o nome da função\n",
    "    function_name = tool_call.function.name\n",
    "    # pegar a função \n",
    "    function_to_call = funcoes_disponiveis[function_name]\n",
    "    # argumentos da função\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    # chamando a função:\n",
    "    function_response = function_to_call(\n",
    "                local=function_args.get(\"local\"),\n",
    "                unidade=function_args.get(\"unidade\"),\n",
    "            )\n",
    "    # adicionando a resposta da função à lista de mensagens\n",
    "    messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id, # importante passar o id da chamada\n",
    "                \"role\": \"tool\", # informamos que é uma mensagem de tool (diferente do que fizemos até agora com user e assistant)\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )\n",
    "# nova requisição\n",
    "quarta_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages,\n",
    "        max_tokens=100, \n",
    "        temperature=0,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualmente, as temperaturas são as seguintes:\n",
      "\n",
      "- São Paulo: 32°C\n",
      "- Porto Alegre: 25°C\n",
      "- Rio de Janeiro: 35°C\n"
     ]
    }
   ],
   "source": [
    "print(quarta_resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parâmetros fora do escopo das funções\n",
    "E o que acontece se adicionarmos uma cidade fora do escopo da função?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adicionando uma nova request com cidade desconhecida na função\n",
    "messages.append({'role':'user','content':'E qual a temperatura em Penápolis?'})\n",
    "\n",
    "# fazendo a solicitação: \n",
    "resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages,\n",
    "        max_tokens=100, \n",
    "        temperature=0,\n",
    "        tools=tools,\n",
    "        tool_choice='auto'\n",
    "    )\n",
    "\n",
    "message_resp = resposta.choices[0].message\n",
    "tool_calls = message_resp.tool_calls\n",
    "\n",
    "# adicionando a mensagem de chamada das requisições\n",
    "messages.append(message_resp)\n",
    "\n",
    "# chamando as funções\n",
    "for tool_call in tool_calls:\n",
    "    # pegar o nome da função\n",
    "    function_name = tool_call.function.name\n",
    "    # pegar a função \n",
    "    function_to_call = funcoes_disponiveis[function_name]\n",
    "    # argumentos da função\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    # chamando a função:\n",
    "    function_response = function_to_call(\n",
    "                local=function_args.get(\"local\"),\n",
    "                unidade=function_args.get(\"unidade\"),\n",
    "            )\n",
    "    # adicionando a resposta da função à lista de mensagens\n",
    "    messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id, # importante passar o id da chamada\n",
    "                \"role\": \"tool\", # informamos que é uma mensagem de tool (diferente do que fizemos até agora com user e assistant)\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )\n",
    "# nova requisição\n",
    "quarta_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages,\n",
    "        max_tokens=100, \n",
    "        temperature=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualmente, não consegui obter a temperatura em Penápolis. Se precisar de informações sobre outra localidade ou mais detalhes, estou à disposição!\n"
     ]
    }
   ],
   "source": [
    "print(quarta_resposta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumindo\n",
    "1. Criar as funções de apoio \n",
    "1. Adicionar as funções à uma lista `tools`\n",
    "1. Criar um dicionário com `\"function_name\":function`\n",
    "1. Criar o objeto `messages` com a mensagem inicial\n",
    "1. Fazer a requisição com o parâmetro `tools`\n",
    "1. Se houver solicitação do modelo para rodar funções:\n",
    "    1. Adicionar a resposta ao `messages`\n",
    "    1. Rodar as funções para cada chamada `tool_calls` solicitada pelo modelo e adicionar as respostas ao `messages`\n",
    "1. Refazer a requisição\n",
    "1. Ler a resposta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criar as funções de apoio \n",
    "def obter_temperatura_atual(local, unidade=\"celsius\"):\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"rio de janeiro\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Rio de Janeiro\", \"temperatura\": \"35\", \"unidade\": unidade}\n",
    "            )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "            )\n",
    "\n",
    "# 2. Adicionar as funções à uma lista `tools`\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"obter_temperatura_atual\",\n",
    "            \"description\": \"Obtém a temperatura atual em uma dada cidade\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"local\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"O nome da cidade. Ex: São Paulo\",\n",
    "                    },\n",
    "                    \"unidade\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"]\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"local\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "    ]\n",
    "\n",
    "# 3. Criar um dicionário com `function_name:function`\n",
    "funcoes_disponiveis = {\n",
    "        \"obter_temperatura_atual\": obter_temperatura_atual,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualmente, a temperatura é a seguinte:\n",
      "\n",
      "- São Paulo: 32°C\n",
      "- Rio de Janeiro: 35°C\n",
      "- Penápolis: Informação de temperatura não disponível. \n",
      "\n",
      "Se precisar de mais alguma coisa, é só avisar!\n"
     ]
    }
   ],
   "source": [
    "# 4. Criar o objeto `messages` com a mensagem inicial\n",
    "messages = [{'role':'user','content':'Qual a temperatura em São Paulo, no Rio de Janeiro e em Penápolis neste momento?'}]\n",
    "\n",
    "# 5. Fazer a requisição com o parâmetro `tools`\n",
    "resposta = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    tools=tools,\n",
    "    tool_choice='auto',\n",
    "    messages=messages,\n",
    "    max_tokens=100, \n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "message_resp = resposta.choices[0].message\n",
    "tool_calls = message_resp.tool_calls\n",
    "\n",
    "# 6. Se houver solicitação do modelo para rodar funções:\n",
    "if tool_calls:\n",
    "    # 6.1 Adicionar a resposta ao `messages`\n",
    "    messages.append(message_resp)\n",
    "    # 6.2 Rodar as funções para cada chamada `tool_calls` solicitada pelo modelo e ...\n",
    "    for tool_call in tool_calls:\n",
    "        function_name = tool_call.function.name\n",
    "        function_to_call = funcoes_disponiveis[function_name]\n",
    "        function_args = json.loads(tool_call.function.arguments)\n",
    "        function_response = function_to_call(\n",
    "            local=function_args.get(\"local\"),\n",
    "            unidade=function_args.get(\"unidade\"),\n",
    "        )\n",
    "        # ... adicionar as respostas ao `messages`\n",
    "        messages.append(\n",
    "            {\n",
    "                \"tool_call_id\": tool_call.id,\n",
    "                \"role\": \"tool\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# 7. Refazer a requisição\n",
    "nova_resposta = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini-2024-07-18\",\n",
    "        messages=messages,\n",
    "        max_tokens=100, \n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "# 8. Ler a resposta\n",
    "print(nova_resposta.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
